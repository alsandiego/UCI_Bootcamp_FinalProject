{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import os\n",
    "from config import *\n",
    "import time\n",
    "\n",
    "# initialize api instance\n",
    "twitter_api = twitter.Api(consumer_key=consumer_key,\n",
    "                         consumer_secret=consumer_secret,\n",
    "                         access_token_key=access_token_key,\n",
    "                         access_token_secret=access_token_secret,\n",
    "                        cache=None,tweet_mode='compat')\n",
    "\n",
    "#print(consumer_key,consumer_secret,access_token_key,access_token_secret)\n",
    "\n",
    "# test authentication\n",
    "print(twitter_api.VerifyCredentials())\n",
    "# print(twitter_api.GetStatus(126402758403305000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100,\n",
    "                                               lang='en',result_type='recent')\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"text\":status.text, \"label\":None, \"user\":status.user.screen_name, \"time\":status.created_at,\n",
    "                \"search_term\":search_keyword} for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blank testDataSet list\n",
    "testDataSet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test search function\n",
    "search_term = input(\"Enter a search keyword: \")\n",
    "testDataSet.extend(buildTestSet(search_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testDataSet[0:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use only to test twitter api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_fetched = twitter_api.GetSearch('disney', count = 100, lang='en')\n",
    "[{\"text\":status.text, \"label\":None, \"user\":status.user.screen_name} for status in tweets_fetched]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start preprocessing of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "traindata140 = os.path.join(\".\",\"trainingandtestdata_sentiment140\",\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file command\n",
    "train_df = pd.read_csv(traindata140, header=None, usecols=[0,5], names=['polarity of the tweet','text'], \n",
    "                       encoding=\"ISO-8859-1\")\n",
    "train_df.head()\n",
    "# (0 = negative, 2 = neutral, 4 = positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use only to process the test dataset provided by Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata140 = os.path.join(\".\",\"trainingandtestdata_sentiment140\",\"test.csv\")\n",
    "test_df = pd.read_csv(testdata140, header=None, usecols=[0,5],\n",
    "                      names=['polarity of the tweet', 'text'],encoding=\"ISO-8859-1\")\n",
    "conditions_test = [\n",
    "    (test_df['polarity of the tweet'] == 0),\n",
    "    (test_df['polarity of the tweet'] == 2),\n",
    "    (test_df['polarity of the tweet'] == 4)]\n",
    "choices_test = ['negative', 'neutral', 'positive']\n",
    "test_df['label'] = np.select(conditions_test, choices_test)\n",
    "test_df.head()\n",
    "\n",
    "test_ls = test_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use to convert numeric value to Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_negative = train_df.loc[train_df['polarity of the tweet']==0]\n",
    "train_df_positive = train_df.loc[train_df['polarity of the tweet']==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull evenly the amount of training set that will be used and combine them\n",
    "frames = [train_df_negative.iloc[0:12500,:],train_df_positive.iloc[0:12500,:]]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (result['polarity of the tweet'] == 0),\n",
    "    (result['polarity of the tweet'] == 2),\n",
    "    (result['polarity of the tweet'] == 4)]\n",
    "choices = ['negative', 'neutral', 'positive']\n",
    "result['label'] = np.select(conditions, choices)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataframe to dictionary to run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls = result.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "    \n",
    "tweetProcessor = PreProcessTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Training Set\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(train_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Test Set\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to process NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def buildVocabulary(preprocessedTrainingData):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in preprocessedTrainingData:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = buildVocabulary(preprocessedTrainingSet)\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute only if model has to be retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SKLearnClassifier = SklearnClassifier(BernoulliNB()).train(trainingFeatures)\n",
    "SKLearnClassifier = SklearnClassifier(SVC(), sparse=False).train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the test set with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKLearnResultLabels = [SKLearnClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model to pickle for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Pickle\n",
    "import pickle\n",
    "save_classifier = open(\"sklearn_svc_train_25k.pickle\",\"wb\")\n",
    "pickle.dump(SKLearnClassifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use to load pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pickle\n",
    "import pickle\n",
    "classifier_f = open(\"sklearn_svc_train_25k.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the test set with the loaded pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKResultLabelsPickle = [classifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to add space before tweet text to eliminate csv issue with tweets that begins with @\n",
    "# list comprehension\n",
    "test_list_text = [' '+test['text'] for test in testDataSet]\n",
    "# convert twitter time output to real time output\n",
    "test_list_time = [time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(test['time'],'%a %b %d %H:%M:%S +0000 %Y')) \n",
    "for test in testDataSet]\n",
    "test_list_search_term = [test['search_term'] for test in testDataSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = pd.DataFrame(\n",
    "    {'Tweet': test_list_text,\n",
    "     'Time': test_list_time,\n",
    "     'Result': SKResultLabelsPickle,\n",
    "     'Search_Term': test_list_search_term\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list['Score']=np.where((output_list.Result=='negative'), -1, 1)\n",
    "output_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list.to_csv('./outputSKLearn_test.csv',header=True,\n",
    "                              index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
